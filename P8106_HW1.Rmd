---
title: "P8106_HW1"
author:
- "Naomi Simon-Kumar"
- ns3782
date: "2/16/2025"
output:
  pdf_document:
    latex_engine: xelatex
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

## Loading libraries

```{r libraries}

library(ISLR)
library(glmnet)
library(caret)
library(tidymodels)
library(corrplot)
library(ggplot2)
library(plotmo)
library(ggrepel)
library(pls)
library(knitr)

```

## Question (a): Lasso Model

To start, we load the training and testing data and subsequently set a seed for reproducibility.

Next, we initialise 10-fold cross-validation to partition the training data into 10 equal subsets. This allows training the model on 9 folds while validating on the final fold. This ensures we evaluate the performance of the model, while avoiding overfitting.

```{r}

# Load training and testing data

training_data <- read.csv("housing_training.csv")
testing_data <- read.csv("housing_test.csv")

set.seed(29)  # Ensure results are reproducible

# Using 10 fold cross-validation

ctrl1 <- trainControl(method = "cv", number = 10)

```

Next, we proceed to fit a lasso regression model using the training data. Sale_Price is the outcome variable, with all other variables as predictors. The lasso model is tuned over a sequence of 100 lambda values ranging from exp(6) to exp(-5).

```{r}

set.seed(29)  # Ensure results are reproducible

# Fit the Lasso model

lasso.fit  <- train(
  Sale_Price ~ .,
  data = training_data,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, 
                         lambda = exp(seq(6, -5, length = 100))),
  trControl = ctrl1
)

# Plot

plot(lasso.fit, xTrans = log)

```

Based on the plot, it appears as though the optimal lambda value is around exp(4), as this is where the RMSE is minimised. Higher lambda values (i.e., greater penalisation) appear to result in poorer model performance, likely due to excessive shrinkage forcing too many coefficients to zero, leading to underfitting.

```{r}

set.seed(29)  # Ensure results are reproducible

# Find optimal tuning parameter

lasso.fit$bestTune

# Extracting coefficients for each predictor, at the optimal lambda

coef(lasso.fit$finalModel, lasso.fit$bestTune$lambda)

```

Note that at the optimal lambda value, most of the predictors remain in the model. However, some are shrunk to zero (i.e., Second_Flr_SF, Fireplace_QuGood) during the variable selection process, and removed from the model. Therefore, this final model includes **37 predictors**.

```{r}

set.seed(29)  # Ensure results are reproducible

# Finding RMSE

lasso_preds <- predict(lasso.fit, newdata = testing_data)  
lasso_rmse <- sqrt(mean((lasso_preds - testing_data$Sale_Price)^2))
print(lasso_rmse)

```

For the lasso model, the optimal tuning parameter lambda is **68.18484**, representing where RMSE is minimised. The test error (RMSE) at this lambda is **20969.2**.

```{r}

set.seed(29)  # Ensure results are reproducible

# Using 1se cross-validation. 
# Code from: https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/oneSE

ctrl_1se <- trainControl(
  method = "cv",
  selectionFunction = "oneSE"  
)

# Fit the lasso model using 1se

lasso_1se_fit <- train(
  Sale_Price ~ .,
  data = training_data,
  method = "glmnet",
  tuneGrid = expand.grid(
    alpha = 1,
    lambda = exp(seq(6, -5, length = 100))  
  ),
  trControl = ctrl_1se
)

# Optimal lambda using 1SE 

lasso_lambda_1se <- lasso_1se_fit$bestTune$lambda
print(lasso_lambda_1se)

# Extracting coefficients for each predictor, at the optimal lambda

coef(lasso_1se_fit$finalModel, s = lasso_lambda_1se)

```

Using the 1SE rule, the optimal lambda is **403.4288**. During the variable selection process, variables Second_Flr_SF, Fireplace_QuNo_Fireplace, and Exter_QualGood are removed from the model.
When the 1SE rule is applied, there are **36 predictors** included in the model, which is 1 fewer than the original lasso model.

## Question (b): Elastic Net

To fit the elastic net model, I began with a wide lambda range.

```{r}

# Set seed to ensure reproducibility

set.seed(16)

# Fit elastic net model
# Tuning the different lambda ranges

enet.fit <- train(Sale_Price ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(6, -5, length = 100))),
                  trControl = ctrl1)

# Results

print(enet.fit$bestTune)

# Cross validation plot

plot(enet.fit, xTrans = log)

```

After reviewing the cross-validation plot, I refined the lambda range.

```{r}

# Set seed to ensure reproducibility

set.seed(16)

# Adjusting

enet.fit <- train(Sale_Price ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21), 
                                         lambda = exp(seq(5.7, -2, length = 100))),
                  trControl = ctrl1)

# Cross validation plot

plot(enet.fit, xTrans = log)

# Optimal lambda

print(enet.fit$bestTune)

```

The cross validation plot shows the RMSE values were fairly stable at lower regularisation values, but increasing steeply when log(lambda) â‰ˆ 6. Therefore, the selected tuning parameters are **alpha = 0.1** and **lambda = 298.8674**.

```{r}

# Set seed to ensure reproducibility

set.seed(16)

# Predictions using testing dataset

enet.pred <- predict(enet.fit, newdata = testing_data)

# Test error

enet_test_mse <- mean((enet.pred - testing_data$Sale_Price)^2)

# Results

print(enet_test_mse)

```

From this, the test error of the model is **440832286**.

```{r}

# Set seed to ensure reproducibility

set.seed(16)


```

Yes, it is possible to apply the 1SE rule to selecting tuning parameters for elastic net. The elastic net method includes penalties from both ridge regression and lasso (the mixing parameter alpha that determines the balance between ridge and lasso penalties, and the overall regularisation strength lambda). The 1SE rule is defined as the most regularised model such that error is within one standard error of the minimum.

Therefore, using the 1SE rule, it is possible to select the most regularised model (i.e., the largest lambda) for each alpha value that has error within one standard error of the minimum, then compare across different alpha values to give the effective regularisation via the ridge-type penalty and feature selection via the lasso penalty, as determined by cross-validation.

## Question (c): Partial least squares

I proceeded with fitting the partial least squares model.

```{r}

# Set seed for reproducibility

set.seed(29)

# Fitting the model

pls_mod <- plsr(Sale_Price ~ .,
                data = training_data,
                scale = TRUE,
                validation = "CV")

summary(pls_mod)

# Determine the optimal number of components

cv_mse <- RMSEP(pls_mod)

ncomp_cv <- which.min(cv_mse$val[1,,]) - 1

# Optimal number of components

print(ncomp_cv)

```

Based on the computation above, the optimal number of components is **8**. 

```{r}

# Set seed for reproducibility

set.seed(29)

# Calculate Test MSE

y2 <- testing_data$Sale_Price

predy2_pls <- predict(pls_mod, newdata = testing_data,
                      ncomp = ncomp_cv)

pls_test_mse <- mean((y2 - predy2_pls)^2)

print(pls_test_mse)

```

The test error for this model is **440217938**.

## Question (d): Choose the best model for predicting the response and explain your choice.


```{r}

# Comparison table of models
# Code from: https://bookdown.org/yihui/rmarkdown-cookbook/kable.html

comparison_table <- tibble(
  Model = c("LASSO (min)", "Elastic Net", "Partial Least Square Regression"),
  Test_Error = c(lasso_rmse, enet_test_mse, pls_test_mse)
)

# Using kable to present table

knitr::kable(comparison_table)

```


## Question (e): Retrain model using glmnet


```{r}


```

